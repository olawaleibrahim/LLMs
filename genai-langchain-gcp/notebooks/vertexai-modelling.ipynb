{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "llm = VertexAI(model_name=\"gemini-1.5-pro\")\n",
    "print(llm.invoke(\"Which question can you answer?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Write a poem about Google CLoud and LangCHain\"\n",
    "for chunk in llm.stream(task):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "\n",
    "for chunk in llm.stream(\n",
    "    \"Write a poem about Sango Ota and Nigeria\",\n",
    "    temperature=0.9,\n",
    "    max_output_tokens=100,\n",
    "    stop=[\".\"],\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH\n",
    "        : HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n",
    "    }\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Extract {entities} entities from the item description: \\n{description}\\n.\n",
    "    Answer with a valid json as an output.\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"\"\"It costs #=900 pounds with 16GB of RAM. Buy Pixel 8 and get up to £355 back with eligible phone trade-in. \\n\n",
    "Offer starts on 05/02/2025 at 00:00 GMT and ends on 12/03/2025 at 23:59 GMT.  \\n\n",
    "Typical representative amount: £355 for \"\"\"\n",
    "result = chain.invoke(\n",
    "    {\"entities\": \"price, RAM\", \"description\": description}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "message = BaseMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    type=\"human\",\n",
    "    additional_kwargs={\"chapter\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "chat_model = ChatVertexAI(model_name=\"gemini-1.5-pro\")\n",
    "message = HumanMessage(content=\"What is new today?\")\n",
    "answer = chat_model.invoke([message])\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message2 = HumanMessage(content=\"What is the cost of two pineapples from ASDA?\")\n",
    "answer2 = chat_model.invoke(\n",
    "    [message, answer, message2], temperature=0.9\n",
    ")\n",
    "print(answer2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer.response_metadata[\"usage_metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that helps extract entities from product descriptions.\"\n",
    "                \"You always respond in a json format.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Extract the following entities: \\n {entities} \\n from the item's description: \\n {description}\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | chat_model | JsonOutputParser()\n",
    "result = chain.invoke({\"entities\": \"price, RAM\",\n",
    "                       \"description\": description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai.callbacks import VertexAICallbackHandler\n",
    "handler = VertexAICallbackHandler()\n",
    "config = {\n",
    "    \"callbacks\": [handler]\n",
    "}\n",
    "result = chain.invoke(\n",
    "    {\"entities\": \"price, RAM\",\n",
    "     \"description\": description}, config=config\n",
    ")\n",
    "print(handler.prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_llm = VertexAI(\n",
    "    model_name=\"gemini-1.5-flash\", max_output_tokens=2048)\n",
    "print(fast_llm.invoke(\"Generate a python script to sort a list of integer numbers. FAST!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def merge_sort(arr):\n",
    "  \"\"\"\n",
    "  Implements the merge sort algorithm for efficient sorting.\n",
    "\n",
    "  Args:\n",
    "    arr: The list of integers to sort.\n",
    "\n",
    "  Returns:\n",
    "    The sorted list.\n",
    "  \"\"\"\n",
    "  if len(arr) <= 1:\n",
    "    return arr\n",
    "  mid = len(arr) // 2\n",
    "  left_half = merge_sort(arr[:mid])\n",
    "  right_half = merge_sort(arr[mid:])\n",
    "  return merge(left_half, right_half)\n",
    "\n",
    "def merge(left, right):\n",
    "  \"\"\"\n",
    "  Merges two sorted lists into a single sorted list.\n",
    "\n",
    "  Args:\n",
    "    left: The first sorted list.\n",
    "    right: The second sorted list.\n",
    "\n",
    "  Returns:\n",
    "    The merged sorted list.\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  i = j = 0\n",
    "  while i < len(left) and j < len(right):\n",
    "    if left[i] <= right[j]:\n",
    "      result.append(left[i])\n",
    "      i += 1\n",
    "    else:\n",
    "      result.append(right[j])\n",
    "      j += 1\n",
    "  result += left[i:]\n",
    "  result += right[j:]\n",
    "  return result\n",
    "\n",
    "# Generate a large list of random integers\n",
    "import random\n",
    "numbers = [random.randint(0, 100000) for _ in range(100000)]\n",
    "\n",
    "# Measure sorting time using merge sort\n",
    "start_time = time.time()\n",
    "sorted_numbers = merge_sort(numbers.copy())\n",
    "end_time = time.time()\n",
    "print(\"Merge Sort time:\", end_time - start_time)\n",
    "\n",
    "# Measure sorting time using Python's built-in sort\n",
    "start_time = time.time()\n",
    "numbers.sort()  # Python's built-in Timsort\n",
    "end_time = time.time()\n",
    "print(\"Python's built-in sort time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_vertexai import VertexAIModelGarden\n",
    "# llama_endpoint_id = \"meta/llama-3.1-8b-instruct-maas\"\n",
    "# project = \"gcp-langchain-450916\"\n",
    "# location = \"us-central1\"\n",
    "# llama_model = VertexAIModelGarden(\n",
    "#     endpoint_id=llama_endpoint_id,\n",
    "#     project=project,\n",
    "#     location=location,\n",
    "# )\n",
    "# output = llama_model.invoke([\"How much is 2+2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"\"\"---INSTRUCTION--- \\nYou are an intelligent assistant that helps marketers \n",
    "    write great copy for campaigns on our website,     which sells premium ceiling\n",
    "      fans to design-conscious customers. Please create campaign copy \n",
    "      (a slogan, a tagline, a short description, and three calls-to-action) based on keywords. \n",
    "      Use the information from your context to choose the right products to advertise.\n",
    "        Follow the examples below to ensure that you follow company branding standards.\\n\"\"\"\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"keywords\": \"\"\"best fan for hot summer days, powerful, cozy, wood tone, enjoy cold drink\"\"\",\n",
    "        \"response\": (\n",
    "            \"Slogan:  Breeze 4000: Feel the Difference.\\n\"\n",
    "            \"Tagline: Design, Comfort, Performance – The Ultimate Summer Upgrade.\\n\"\n",
    "            \"Short Description:  Beat the heat in style with the Breeze 4000. Its sleek wood-tone design and \"\n",
    "            \"whisper-quiet operation create the perfect oasis for enjoying a cool drink on those hot summer days.\\n\"\n",
    "            \"Call to action: 1/ Experience the Breeze 4000 difference today.  (Emphasizes the unique qualities)\\n\"\n",
    "            \"2/ Upgrade your summer. Shop the Breeze 4000 now. (Creates a sense of urgency)\\n\"\n",
    "            \"3/ Find your perfect Breeze 4000 style. (Focus on design and personalization)\"\n",
    "        )\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt_template = \"---CONTEXT---\\n{context}\\n------KEYWORDS FOR CREATING COPY---\\n{keywords}\\n---EXAMPLES---\\n{examples}\"\n",
    "context = [\n",
    "    {\n",
    "        \"name\": \"Whirlwind BreezeMaster 3000\",\n",
    "        \"performanceRating\": \"high\",\n",
    "        \"outdoor\": True,\n",
    "        \"powerSource\": \"electric\",\n",
    "        \"price\": 249.99\n",
    "    }\n",
    "]\n",
    "keywords = \"best fan for dry heat, powerful, outdoor, porch, affordable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"keywords\", \"response\"],\n",
    "    template=\"Example keywords: \\n{keywords}\\nExample response:\\n{response}\"\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=instruction,\n",
    "    suffix=\"---CONTEXT---\\n{context}\\n---KEYWORDS FOR CREATING COPY---\\n{keywords}\\n\",\n",
    "    input_variables=[\"context\", \"keywords\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI(model_name=\"gemini-pro\")\n",
    "response = (prompt | llm).invoke({\"context\": context, \"keywords\": keywords})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
